{"cells":[{"cell_type":"code","source":["# basic libraries \nfrom ftplib import FTP\nimport os\nimport py7zr\nfrom delta.tables import *\nfrom pyspark.sql import functions as psf\nimport icecream\nimport pandas as pd \nfrom pyspark.sql.types import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cbb17d9-f8da-4896-a9b7-cc314875e30b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["### ftp parameters \nftpServerUrl = 'ftpupload.net'\nftpServerPort=21\nftpPath='/htdocs/nhsgp/'\nftpUserName ='topsecret***'\nftpPassword ='topsecret***'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d22eff65-0bda-4e25-8f6c-62553dfd8952"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#get ftp file list \nlocalPath = '/dbfs/'\ntry: \n  ftp= FTP(ftpServerUrl)\n  ftp.login(user=ftpUserName, passwd=ftpPassword)\n  ftp.cwd(ftpPath)\n  \n  files = [ f for f in ftp.nlst() if f.endswith('.7z') or f.endswith('.csv')or f.endswith('.json') or 'PDPI BNFT.7z' in f  or 'BNF Snomed Mapping data' in f] # f.endswith('.7z') #testing: f=='T201606PDPI+BNFT.7z' \n   \n  print('downloading:')   \n  \n  for f in files:\n    print(f)\n    localfile = open(localPath+f, \"wb\") #open(f,'wb')\n    ftp.retrbinary('RETR '+f, localfile.write,1024)\n    localfile.close() \n    \nexcept :\n  print(\"FTP Error: \")\nftp.quit() \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97016874-90b4-4e20-942f-874a3ebea6e7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">downloading:\nBNF Snomed Mapping data.xlsx\nT201901PDPI BNFT.7z.001\nT201901PDPI BNFT.7z.002\nT201901PDPI BNFT.7z.003\nT201901PDPI BNFT.7z.004\nT201901PDPI BNFT.7z.005\nT201901PDPI BNFT.7z.006\nT201901PDPI BNFT.7z.007\nT201901PDPI BNFT.7z.008\nT201901PDPI BNFT.7z.009\nT201902PDPI BNFT.7z.001\nT201902PDPI BNFT.7z.002\nT201902PDPI BNFT.7z.003\nT201902PDPI BNFT.7z.004\nT201902PDPI BNFT.7z.005\nT201902PDPI BNFT.7z.006\nT201902PDPI BNFT.7z.007\nT201902PDPI BNFT.7z.008\nT201902PDPI BNFT.7z.009\nT201903PDPI BNFT.7z.001\nT201903PDPI BNFT.7z.002\nT201903PDPI BNFT.7z.003\nT201903PDPI BNFT.7z.004\nT201903PDPI BNFT.7z.005\nT201903PDPI BNFT.7z.006\nT201903PDPI BNFT.7z.007\nT201903PDPI BNFT.7z.008\nT201903PDPI BNFT.7z.009\nT201912ADDR BNFTHeader.csv\nT201912CHEM SUBS.csv\ncolumn_remapping.json\nOut[30]: &#39;221-Goodbye. You uploaded 0 and downloaded 264641 kbytes.\\n221 Logout.&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">downloading:\nBNF Snomed Mapping data.xlsx\nT201901PDPI BNFT.7z.001\nT201901PDPI BNFT.7z.002\nT201901PDPI BNFT.7z.003\nT201901PDPI BNFT.7z.004\nT201901PDPI BNFT.7z.005\nT201901PDPI BNFT.7z.006\nT201901PDPI BNFT.7z.007\nT201901PDPI BNFT.7z.008\nT201901PDPI BNFT.7z.009\nT201902PDPI BNFT.7z.001\nT201902PDPI BNFT.7z.002\nT201902PDPI BNFT.7z.003\nT201902PDPI BNFT.7z.004\nT201902PDPI BNFT.7z.005\nT201902PDPI BNFT.7z.006\nT201902PDPI BNFT.7z.007\nT201902PDPI BNFT.7z.008\nT201902PDPI BNFT.7z.009\nT201903PDPI BNFT.7z.001\nT201903PDPI BNFT.7z.002\nT201903PDPI BNFT.7z.003\nT201903PDPI BNFT.7z.004\nT201903PDPI BNFT.7z.005\nT201903PDPI BNFT.7z.006\nT201903PDPI BNFT.7z.007\nT201903PDPI BNFT.7z.008\nT201903PDPI BNFT.7z.009\nT201912ADDR BNFTHeader.csv\nT201912CHEM SUBS.csv\ncolumn_remapping.json\nOut[30]: &#39;221-Goodbye. You uploaded 0 and downloaded 264641 kbytes.\\n221 Logout.&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# extract single 7z files \ndef extract7zFile(filePathAndName, fileRemoved):  \n  if(filePathAndName.endswith('.7z')):\n    archive = py7zr.SevenZipFile(filePathAndName, mode='r')\n    archive.extractall(path='/dbfs/')\n    archive.close()\n    if (fileRemoved):\n      os.remove(filePathAndName)\n  else: \n    print('Not a 7Z file!')\n    \n    \n    \ndef extract7zMultiVolumn(filePath, fileName, fileRemoved):  \n  mulitVolumnFiles = sorted([ f for f in os.listdir(filePath) if fileName in f])\n\n  tempFilePath = filePath +'tempAll.7z'\n  with open(tempFilePath, 'ab') as outfile:  # append in binary mode\n    for fname in mulitVolumnFiles:\n      with open(filePath+fname, 'rb') as infile:        # open in binary mode also\n#         print(filePath+fname)\n        outfile.write(infile.read())\n  extract7zFile(tempFilePath, fileRemoved)\n  #remove multivolumn files \n  if(fileRemoved):\n    for f in mulitVolumnFiles:\n      os.remove(filePath+f)\n      \n      \n#convert xlsx to csv due to poor performance of spark-excel \ndef convertXlsxToCsv(filepath, filename):\n  tempInputPath= os.path.join(filepath, filename)\n  tmpXpd =  pd.read_excel(tempInputPath, sheet_name='November 20',engine='openpyxl')\n  tmpFilename =  str.replace(filename, '.xlsx','')\n  tmpOutxpath = localPath + tmpFilename +'.csv'\n  os.remove(tempInputPath)\n  tmpXpd.to_csv(tmpOutxpath)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1727b74-df38-4244-a7c7-53d3eb54e04b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["filePath= '/dbfs/'\nfileName = 'T201901PDPI BNFT'\nextract7zMultiVolumn(filePath,fileName,True)\n\nfileName = 'T201902PDPI BNFT'\nextract7zMultiVolumn(filePath,fileName,True)\n\nfileName = 'T201903PDPI BNFT'\nextract7zMultiVolumn(filePath,fileName,True)\n\n\n#convert xlsx files to csv \nfor f in os.listdir('/dbfs'):\n  if(f.endswith('.xlsx')):\n    convertXlsxToCsv(filePath, f)\n    \n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d76eb23-6b59-4902-a44f-67be359bd1ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#move files from localfolder into dbfs tmp folder \nfor f in os.listdir('/dbfs'):\n  if(f.endswith('.csv') or f.endswith('.json')):    \n    dbutils.fs.cp(\"file:/dbfs/\"+f,\"dbfs:/tmp/\")\n    os.remove('/dbfs/'+f)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9876478e-2508-4547-aedd-ff0c674904cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#check path \ndef checkPathExist(checkPath):\n  try:\n    dbutils.fs.ls(checkPath)\n    return True\n  except Exception as e:\n    if 'java.io.FileNotFoundException' in str(e):\n      return False\n\n#save to table \ndef saveIntoLandingTable(tableName, inputDf):\n  print(\"saving table {}...\".format(tableName))\n  spark.sql(\"use nhsgp\")\n  spark.sql(\"DROP TABLE IF EXISTS {}\".format(tableName))\n  tempPath = \"/user/hive/warehouse/{}\".format(tableName)\n  if(checkPathExist(tempPath)):\n    dbutils.fs.rm(tempPath, True)\n  inputDf.write.format(\"delta\").save(tempPath)\n  spark.sql(\"CREATE TABLE {} USING DELTA LOCATION '{}'\".format(tableName, tempPath))\n\n  \n#append fact table   \ndef appenFactTable(tableName, newDf):\n  print(\"saving table {}...\".format(tableName))\n  spark.sql(\"use nhsgp\")\n  tempPath = \"/user/hive/warehouse/{}/\".format(tableName)\n  existingTable  = DeltaTable.forPath(spark, tempPath)\n\n  existingTable.alias(\"old\").merge(\n      newDf.alias(\"new\"),\n      \"1 = 2\") \\\n    .whenNotMatchedInsert(values =\n      {\n        \"SHA\": \"new.SHA\",\n        \"PCT\": \"new.PCT\",\n        \"PRACTICE\": \"new.PRACTICE\",\n        \"BNF_CODE\": \"new.BNF_CODE\",\n        \"BNF_NAME\": \"new.BNF_NAME\",\n        \"ITEMS\": \"new.ITEMS\",\n        \"NIC\": \"new.NIC\",\n        \"ACT_COST\": \"new.ACT_COST\",\n        \"QUANTITY\": \"new.QUANTITY\",        \n        \"PERIOD\": \"new.PERIOD\"\n      }\n    ) \\\n    .execute()\n\n#reading a nested JSON file \ndef getColumnMappingsFromJson(filepath): \n  jdf = spark.read.json(filepath,encoding='utf-8')\n  jdf = jdf.select(\n      psf.array(psf.expr('bnf_code.*')).alias('bnf_code'),\n      psf.array(psf.expr('bnf_name.*')).alias('bnf_name'),\n      psf.array(psf.expr('practice.*')).alias('practice')\n  )\n  jdf = (jdf.withColumn(\"Code_Name_Practice\", psf.explode(psf.arrays_zip(\"bnf_code\", \"bnf_name\",\"practice\")))\n    .select(\"Code_Name_Practice.bnf_code\", \"Code_Name_Practice.bnf_name\", \"Code_Name_Practice.practice\")) \n  return jdf "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6975426-20e6-4a8d-a182-1b72276b6806"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#list temp files and load into landing storage \nrawFiles = [ f for f in dbutils.fs.ls('/tmp') if f.name.endswith('.csv') or f.name.endswith('.json') or f.name.endswith('.xlsx')]\nfactDescriptionFileCounter = 0 \nfor f in rawFiles:\n  print(\"Processing file {}\".format(f.name))\n  if(f.name.endswith('.json')):\n    loadDf = getColumnMappingsFromJson(f.path)\n    saveIntoLandingTable(\"landing_column_mappings\",loadDf)    \n  if('CHEM SUBS' in f.name):\n    loadDf = spark.read.csv(f.path, inferSchema=True, header=True, encoding='UTF-8')\n    loadDf= loadDf.withColumnRenamed(\"CHEM SUB\",\"ChemSub\")\n    saveIntoLandingTable(\"landing_dim_chem\",loadDf)\n  if('ADDR BNF' in f.name):\n    loadDf = spark.read.csv(f.path, inferSchema=True, header=True, encoding='UTF-8')\n    saveIntoLandingTable(\"landing_dim_practices\",loadDf)\n  if('BNF Snomed Mapping data' in f.name):\n    localschema = StructType() \\\n      .add(\"index\",IntegerType(),True) \\\n      .add(\"BNF Code\",StringType(),True) \\\n      .add(\"BNF Name\",StringType(),True) \\\n      .add(\"SNOMED Code\",StringType(),True)\n    \n    loadDf = spark.read.csv(f.path, schema=localschema, header=True, encoding='UTF-8')\n    loadDf= loadDf.withColumnRenamed('BNF Code','BNF_Code')\n    loadDf= loadDf.withColumnRenamed('BNF Name','BNF_Name')\n    loadDf= loadDf.withColumnRenamed('SNOMED Code','SNOMED_Code') \n    \n    saveIntoLandingTable(\"landing_dim_BnfSnomedMapping\",loadDf)\n#   if('BNF Snomed Mapping data' in f.name):\n#     loadDf = spark.read.format(\"com.crealytics.spark.excel\") \\\n#                         .option(\"inferSchema\", \"true\") \\\n#                         .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n#                         .option(\"header\", \"true\") \\\n#                         .option(\"sheetName\", \"November 20\") \\\n#                         .load(f.path)     \n#     loadDf= loadDf.withColumnRenamed('BNF Code','BNF_Code')\n#     loadDf= loadDf.withColumnRenamed('BNF Name','BNF_Name')\n#     loadDf= loadDf.withColumnRenamed('SNOMED Code','SNOMED_Code')       \n#     saveIntoLandingTable(\"landing_dim_BnfSnomedMapping\",loadDf)\n    \n  if('PDPI BNFT.csv' in f.name):\n    print(f.name)\n    loadDf = spark.read.csv(f.path, inferSchema=True, header=True, encoding='UTF-8')\n    loadDf= loadDf.withColumnRenamed(\"BNF CODE\",\"BNF_CODE\")\n    loadDf= loadDf.withColumnRenamed(\"BNF NAME\",\"BNF_NAME\")\n    loadDf= loadDf.withColumnRenamed(\"ACT COST\",\"ACT_COST\")\n    if(factDescriptionFileCounter == 0 ):\n      saveIntoLandingTable(\"landing_fact_predescription\",loadDf)      \n      print(\"Loading fact file No. {}\".format(factDescriptionFileCounter))\n    else:\n      appenFactTable(\"landing_fact_predescription\",loadDf)\n      print(\"Loading fact file No. {}\".format(factDescriptionFileCounter))\n    factDescriptionFileCounter=factDescriptionFileCounter+1\n    \n    # remove imported raw file \n#   dbutils.fs.rm(f.path)    \n "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04f96442-45c6-476a-83e7-9787e691ddad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Processing file BNF Snomed Mapping data.csv\nsaving table landing_dim_BnfSnomedMapping...\nProcessing file T201901PDPI BNFT.csv\nT201901PDPI BNFT.csv\nsaving table landing_fact_predescription...\nLoading fact file No. 0\nProcessing file T201902PDPI BNFT.csv\nT201902PDPI BNFT.csv\nsaving table landing_fact_predescription...\nLoading fact file No. 1\nProcessing file T201903PDPI BNFT.csv\nT201903PDPI BNFT.csv\nsaving table landing_fact_predescription...\nLoading fact file No. 2\nProcessing file T201912ADDR BNFTHeader.csv\nsaving table landing_dim_practices...\nProcessing file T201912CHEM SUBS.csv\nsaving table landing_dim_chem...\nProcessing file column_remapping.json\nsaving table landing_column_mappings...\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Processing file BNF Snomed Mapping data.csv\nsaving table landing_dim_BnfSnomedMapping...\nProcessing file T201901PDPI BNFT.csv\nT201901PDPI BNFT.csv\nsaving table landing_fact_predescription...\nLoading fact file No. 0\nProcessing file T201902PDPI BNFT.csv\nT201902PDPI BNFT.csv\nsaving table landing_fact_predescription...\nLoading fact file No. 1\nProcessing file T201903PDPI BNFT.csv\nT201903PDPI BNFT.csv\nsaving table landing_fact_predescription...\nLoading fact file No. 2\nProcessing file T201912ADDR BNFTHeader.csv\nsaving table landing_dim_practices...\nProcessing file T201912CHEM SUBS.csv\nsaving table landing_dim_chem...\nProcessing file column_remapping.json\nsaving table landing_column_mappings...\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["\n# # clear temp files and tables \n\nspark.sql('use nhsgp')\nspark.sql(\"show tables\").show(truncate=False )\n\nspark.sql(\"select count(1) from landing_fact_predescription\").show()\n# spark.sql(\"select count(1) from landing_dim_practices\").show()\n# spark.sql(\"select count(1) from landing_dim_chem\").show()\n# spark.sql(\"select count(1) from landing_column_mappings\").show()\n# spark.sql(\"select count(1) from landing_dim_chem\").show()\n# spark.sql(\"select count(1) from landing_dim_bnfsnomedmapping\").show()\n\n\n\n# spark.sql(\"drop table landing_fact_predescription \")\n# spark.sql(\"drop table landing_dim_practices \")\n# spark.sql(\"drop table landing_dim_chem \")\n# spark.sql(\"drop table landing_column_mappings\")\n\n\n\n# clean all loading files \n# os.listdir('/dbfs')\n# dbutils.fs.ls('/tmp')\n\n# for f in os.listdir('/dbfs'):\n#   os.remove('/dbfs/'+f)\n# !ls -l '/dbfs'\n\n# dbutils.fs.rm(f.path) \n# rawFiles = [ f for f in dbutils.fs.ls('/tmp') if f.name.endswith('.csv') or f.name.endswith('.json') or f.name.endswith('.xlsx') ]\n# for f in rawFiles:\n#   dbutils.fs.rm(f.path)\n  \n      \ndbutils.fs.ls('/tmp')            \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81c40388-b0ae-4613-8d07-13432922bfcc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------+--------------------------------+-----------+\n|database|tableName                       |isTemporary|\n+--------+--------------------------------+-----------+\n|nhsgp   |landing_column_mappings         |false      |\n|nhsgp   |landing_dim_bnfsnomedmapping    |false      |\n|nhsgp   |landing_dim_chem                |false      |\n|nhsgp   |landing_dim_practices           |false      |\n|nhsgp   |landing_fact_predescription     |false      |\n|nhsgp   |presentation_dim_bnf_snomed     |false      |\n|nhsgp   |presentation_dim_chem           |false      |\n|nhsgp   |presentation_dim_gp             |false      |\n|nhsgp   |presentation_dim_gp_bnf         |false      |\n|nhsgp   |presentation_fact_description_gp|false      |\n+--------+--------------------------------+-----------+\n\n+--------+\n|count(1)|\n+--------+\n|28668451|\n+--------+\n\nOut[36]: [FileInfo(path=&#39;dbfs:/tmp/BNF Snomed Mapping data.csv&#39;, name=&#39;BNF Snomed Mapping data.csv&#39;, size=30372000),\n FileInfo(path=&#39;dbfs:/tmp/T201901PDPI BNFT.csv&#39;, name=&#39;T201901PDPI BNFT.csv&#39;, size=804488478),\n FileInfo(path=&#39;dbfs:/tmp/T201902PDPI BNFT.csv&#39;, name=&#39;T201902PDPI BNFT.csv&#39;, size=772678193),\n FileInfo(path=&#39;dbfs:/tmp/T201903PDPI BNFT.csv&#39;, name=&#39;T201903PDPI BNFT.csv&#39;, size=798371431),\n FileInfo(path=&#39;dbfs:/tmp/T201912ADDR BNFTHeader.csv&#39;, name=&#39;T201912ADDR BNFTHeader.csv&#39;, size=974066),\n FileInfo(path=&#39;dbfs:/tmp/T201912CHEM SUBS.csv&#39;, name=&#39;T201912CHEM SUBS.csv&#39;, size=101637),\n FileInfo(path=&#39;dbfs:/tmp/column_remapping.json&#39;, name=&#39;column_remapping.json&#39;, size=1990407),\n FileInfo(path=&#39;dbfs:/tmp/hive/&#39;, name=&#39;hive/&#39;, size=0)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+--------------------------------+-----------+\ndatabase|tableName                       |isTemporary|\n+--------+--------------------------------+-----------+\nnhsgp   |landing_column_mappings         |false      |\nnhsgp   |landing_dim_bnfsnomedmapping    |false      |\nnhsgp   |landing_dim_chem                |false      |\nnhsgp   |landing_dim_practices           |false      |\nnhsgp   |landing_fact_predescription     |false      |\nnhsgp   |presentation_dim_bnf_snomed     |false      |\nnhsgp   |presentation_dim_chem           |false      |\nnhsgp   |presentation_dim_gp             |false      |\nnhsgp   |presentation_dim_gp_bnf         |false      |\nnhsgp   |presentation_fact_description_gp|false      |\n+--------+--------------------------------+-----------+\n\n+--------+\ncount(1)|\n+--------+\n28668451|\n+--------+\n\nOut[36]: [FileInfo(path=&#39;dbfs:/tmp/BNF Snomed Mapping data.csv&#39;, name=&#39;BNF Snomed Mapping data.csv&#39;, size=30372000),\n FileInfo(path=&#39;dbfs:/tmp/T201901PDPI BNFT.csv&#39;, name=&#39;T201901PDPI BNFT.csv&#39;, size=804488478),\n FileInfo(path=&#39;dbfs:/tmp/T201902PDPI BNFT.csv&#39;, name=&#39;T201902PDPI BNFT.csv&#39;, size=772678193),\n FileInfo(path=&#39;dbfs:/tmp/T201903PDPI BNFT.csv&#39;, name=&#39;T201903PDPI BNFT.csv&#39;, size=798371431),\n FileInfo(path=&#39;dbfs:/tmp/T201912ADDR BNFTHeader.csv&#39;, name=&#39;T201912ADDR BNFTHeader.csv&#39;, size=974066),\n FileInfo(path=&#39;dbfs:/tmp/T201912CHEM SUBS.csv&#39;, name=&#39;T201912CHEM SUBS.csv&#39;, size=101637),\n FileInfo(path=&#39;dbfs:/tmp/column_remapping.json&#39;, name=&#39;column_remapping.json&#39;, size=1990407),\n FileInfo(path=&#39;dbfs:/tmp/hive/&#39;, name=&#39;hive/&#39;, size=0)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43c8fd9d-b7c4-4b52-a13f-a06bff0c11cb"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dataLanding","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2161992318415420}},"nbformat":4,"nbformat_minor":0}
