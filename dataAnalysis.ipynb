{"cells":[{"cell_type":"code","source":["import numpy as np \nimport matplotlib.pyplot as plt  \nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\nimport pyspark.sql.functions  as sfn \nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f154d6-689c-45a0-861c-1c0d8ace6011"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql('use nhsgp')\nspark.sql(\"show tables\").select([\"tableName\"]).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c293c07d-d3bd-4e76-8c79-32a435b89c5c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------------------+\n|tableName                       |\n+--------------------------------+\n|landing_column_mappings         |\n|landing_dim_bnfsnomedmapping    |\n|landing_dim_chem                |\n|landing_dim_practices           |\n|landing_fact_predescription     |\n|presentation_dim_bnf_snomed     |\n|presentation_dim_chem           |\n|presentation_dim_gp             |\n|presentation_dim_gp_bnf         |\n|presentation_fact_description_gp|\n+--------------------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------+\ntableName                       |\n+--------------------------------+\nlanding_column_mappings         |\nlanding_dim_bnfsnomedmapping    |\nlanding_dim_chem                |\nlanding_dim_practices           |\nlanding_fact_predescription     |\npresentation_dim_bnf_snomed     |\npresentation_dim_chem           |\npresentation_dim_gp             |\npresentation_dim_gp_bnf         |\npresentation_fact_description_gp|\n+--------------------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["desPd.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5697864-fe2a-4dc6-84a7-34d49fec0bfb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[29]: [&#39;BNF_CODE&#39;,\n &#39;BNF_NAME&#39;,\n &#39;ITEMS&#39;,\n &#39;NIC&#39;,\n &#39;ACT_COST&#39;,\n &#39;QUANTITY&#39;,\n &#39;PERIOD&#39;,\n &#39;practice&#39;,\n &#39;name&#39;,\n &#39;address1&#39;,\n &#39;address2&#39;,\n &#39;city&#39;,\n &#39;county&#39;,\n &#39;postcode&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[29]: [&#39;BNF_CODE&#39;,\n &#39;BNF_NAME&#39;,\n &#39;ITEMS&#39;,\n &#39;NIC&#39;,\n &#39;ACT_COST&#39;,\n &#39;QUANTITY&#39;,\n &#39;PERIOD&#39;,\n &#39;practice&#39;,\n &#39;name&#39;,\n &#39;address1&#39;,\n &#39;address2&#39;,\n &#39;city&#39;,\n &#39;county&#39;,\n &#39;postcode&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["\ndesPd = spark.sql('select * from presentation_fact_description_gp')\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(desPd) for column in list(['BNF_CODE',  'practice', 'city', 'county']) ]\n\n\npipeline = Pipeline(stages=indexers)\ndf_r = pipeline.fit(desPd).transform(desPd)\n\ndf_r =df_r.dropna()\n\n# df_r.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"911fed23-bcb9-4bc1-a54e-61f7fa7be79d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_r.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82601d11-4954-422a-bd54-8b51ee2d30f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------+--------------------+-----+-------+--------+--------+------+--------+--------------------+--------------------+------------------+----+-----------+--------+--------------+--------------+----------+------------+\n|       BNF_CODE|            BNF_NAME|ITEMS|    NIC|ACT_COST|QUANTITY|PERIOD|practice|                name|            address1|          address2|city|     county|postcode|BNF_CODE_index|practice_index|city_index|county_index|\n+---------------+--------------------+-----+-------+--------+--------+------+--------+--------------------+--------------------+------------------+----+-----------+--------+--------------+--------------+----------+------------+\n|0601012V0BBABAB|Ins Lantus_100u/m...|    1|  27.92|   25.95|     1.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        2698.0|         281.0|     228.0|        18.0|\n|0601012V0BBAEAD|Ins Lantus SoloSt...|   13| 717.63|  666.88|    95.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         133.0|         281.0|     228.0|        18.0|\n|0601012V0BCAAAE|Toujeo_300u/ml 1....|    4| 320.26|  297.69|    29.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         712.0|         281.0|     228.0|        18.0|\n|0601012V0BDACAD|Abasaglar KwikPen...|    4| 211.68|  196.71|    30.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1030.0|         281.0|     228.0|        18.0|\n|0601012W0BBABAB|Ins NovoMix 30_Fl...|   15|1046.15|  972.12|   175.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         212.0|         281.0|     228.0|        18.0|\n|0601012X0AAAAAA|Ins Detemir_100u/...|    1|  168.0|  156.09|    20.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        2551.0|         281.0|     228.0|        18.0|\n|0601012X0BBAAAA|Ins Levemir_Penfi...|    8|  546.0|  507.36|    65.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         546.0|         281.0|     228.0|        18.0|\n|0601012X0BBABAB|Ins Levemir_FlexP...|   12|  672.0|  624.48|    80.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         274.0|         281.0|     228.0|        18.0|\n|0601012Z0AAAAAA|Ins Degludec_100u...|    1|   93.2|    86.6|    10.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        2998.0|         281.0|     228.0|        18.0|\n|0601012Z0BBAAAA|Ins Tresiba_Penfi...|    1|   46.6|   43.31|     5.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1612.0|         281.0|     228.0|        18.0|\n|0601012Z0BBABAB|Ins Tresiba_FlexT...|    4|  233.0|  216.52|    25.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         611.0|         281.0|     228.0|        18.0|\n|0601021A0AAAAAA| Glimepiride_Tab 2mg|    1|   7.39|    6.98|   112.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1069.0|         281.0|     228.0|        18.0|\n|0601021M0AAAAAA| Gliclazide_Tab 80mg|  141| 218.99|   212.1|  7859.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|          80.0|         281.0|     228.0|        18.0|\n|0601021M0AAAMAM|Gliclazide_Tab 30...|   23|  33.72|   33.81|   336.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         443.0|         281.0|     228.0|        18.0|\n|0601021M0AAARAR| Gliclazide_Tab 40mg|   11|   17.4|    17.1|   308.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         154.0|         281.0|     228.0|        18.0|\n|0601021M0AAATAT|Gliclazide_Tab 60...|    1|  19.08|   17.74|   112.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1184.0|         281.0|     228.0|        18.0|\n|0601022B0AAABAB|Metformin HCl_Tab...|  204| 549.15|  517.32| 18305.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|          40.0|         281.0|     228.0|        18.0|\n|0601022B0AAADAD|Metformin HCl_Tab...|   28|  50.71|   49.46|  2075.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         203.0|         281.0|     228.0|        18.0|\n|0601022B0AAARAR|Metformin HCl_Ora...|    5|  89.73|   83.53|  2000.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1071.0|         281.0|     228.0|        18.0|\n|0601022B0AAASAS|Metformin HCl_Tab...|    1|    4.0|    3.73|    56.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         227.0|         281.0|     228.0|        18.0|\n+---------------+--------------------+-----+-------+--------+--------+------+--------+--------------------+--------------------+------------------+----+-----------+--------+--------------+--------------+----------+------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+--------------------+-----+-------+--------+--------+------+--------+--------------------+--------------------+------------------+----+-----------+--------+--------------+--------------+----------+------------+\n       BNF_CODE|            BNF_NAME|ITEMS|    NIC|ACT_COST|QUANTITY|PERIOD|practice|                name|            address1|          address2|city|     county|postcode|BNF_CODE_index|practice_index|city_index|county_index|\n+---------------+--------------------+-----+-------+--------+--------+------+--------+--------------------+--------------------+------------------+----+-----------+--------+--------------+--------------+----------+------------+\n0601012V0BBABAB|Ins Lantus_100u/m...|    1|  27.92|   25.95|     1.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        2698.0|         281.0|     228.0|        18.0|\n0601012V0BBAEAD|Ins Lantus SoloSt...|   13| 717.63|  666.88|    95.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         133.0|         281.0|     228.0|        18.0|\n0601012V0BCAAAE|Toujeo_300u/ml 1....|    4| 320.26|  297.69|    29.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         712.0|         281.0|     228.0|        18.0|\n0601012V0BDACAD|Abasaglar KwikPen...|    4| 211.68|  196.71|    30.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1030.0|         281.0|     228.0|        18.0|\n0601012W0BBABAB|Ins NovoMix 30_Fl...|   15|1046.15|  972.12|   175.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         212.0|         281.0|     228.0|        18.0|\n0601012X0AAAAAA|Ins Detemir_100u/...|    1|  168.0|  156.09|    20.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        2551.0|         281.0|     228.0|        18.0|\n0601012X0BBAAAA|Ins Levemir_Penfi...|    8|  546.0|  507.36|    65.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         546.0|         281.0|     228.0|        18.0|\n0601012X0BBABAB|Ins Levemir_FlexP...|   12|  672.0|  624.48|    80.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         274.0|         281.0|     228.0|        18.0|\n0601012Z0AAAAAA|Ins Degludec_100u...|    1|   93.2|    86.6|    10.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        2998.0|         281.0|     228.0|        18.0|\n0601012Z0BBAAAA|Ins Tresiba_Penfi...|    1|   46.6|   43.31|     5.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1612.0|         281.0|     228.0|        18.0|\n0601012Z0BBABAB|Ins Tresiba_FlexT...|    4|  233.0|  216.52|    25.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         611.0|         281.0|     228.0|        18.0|\n0601021A0AAAAAA| Glimepiride_Tab 2mg|    1|   7.39|    6.98|   112.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1069.0|         281.0|     228.0|        18.0|\n0601021M0AAAAAA| Gliclazide_Tab 80mg|  141| 218.99|   212.1|  7859.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|          80.0|         281.0|     228.0|        18.0|\n0601021M0AAAMAM|Gliclazide_Tab 30...|   23|  33.72|   33.81|   336.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         443.0|         281.0|     228.0|        18.0|\n0601021M0AAARAR| Gliclazide_Tab 40mg|   11|   17.4|    17.1|   308.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         154.0|         281.0|     228.0|        18.0|\n0601021M0AAATAT|Gliclazide_Tab 60...|    1|  19.08|   17.74|   112.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1184.0|         281.0|     228.0|        18.0|\n0601022B0AAABAB|Metformin HCl_Tab...|  204| 549.15|  517.32| 18305.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|          40.0|         281.0|     228.0|        18.0|\n0601022B0AAADAD|Metformin HCl_Tab...|   28|  50.71|   49.46|  2075.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         203.0|         281.0|     228.0|        18.0|\n0601022B0AAARAR|Metformin HCl_Ora...|    5|  89.73|   83.53|  2000.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|        1071.0|         281.0|     228.0|        18.0|\n0601022B0AAASAS|Metformin HCl_Tab...|    1|    4.0|    3.73|    56.0|201903|  G81070|TRINITY MEDICAL C...|TRINITY MEDICAL C...|1 GOLDSTONE VILLAS|HOVE|EAST SUSSEX| BN3 3AT|         227.0|         281.0|     228.0|        18.0|\n+---------------+--------------------+-----+-------+--------+--------+------+--------+--------------------+--------------------+------------------+----+-----------+--------+--------------+--------------+----------+------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# desPd = spark.sql('select * from presentation_fact_description_gp')\n\n# features =   ('BNF_CODE_index', \n#  'ITEMS', \n#  'ACT_COST',\n#  'QUANTITY', \n#  'practice_index', \n#  'city_index',\n#  'county_index')\n\n\nfeatures =   (\n 'ACT_COST',\n 'QUANTITY')\n\n\nassembler = VectorAssembler(inputCols=features,outputCol=\"features\")\n\ndataset=assembler.transform(desPd)\ndataset.select(\"features\").show(truncate=False)\n\n\n# Trains a k-means model\nkmeans = KMeans().setK(2).setSeed(1)\nmodel = kmeans.fit(dataset)\n\n# Make predictions\npredictions = model.transform(dataset)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n\n\n# # Evaluate clustering.\n# cost = model.computeCost(dataset)\n# print(\"Within Set Sum of Squared Errors = \" + str(cost))\n\n# Shows the result.\nprint(\"Cluster Centers: \")\nctr=[]\ncenters = model.clusterCenters()\nfor center in centers:\n    ctr.append(center)\n    print(center)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a49904c-ab04-4901-9af7-27a86a2a4391"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------------+\n|features        |\n+----------------+\n|[25.95,1.0]     |\n|[666.88,95.0]   |\n|[297.69,29.0]   |\n|[196.71,30.0]   |\n|[972.12,175.0]  |\n|[156.09,20.0]   |\n|[507.36,65.0]   |\n|[624.48,80.0]   |\n|[86.6,10.0]     |\n|[43.31,5.0]     |\n|[216.52,25.0]   |\n|[6.98,112.0]    |\n|[212.1,7859.0]  |\n|[33.81,336.0]   |\n|[17.1,308.0]    |\n|[17.74,112.0]   |\n|[517.32,18305.0]|\n|[49.46,2075.0]  |\n|[83.53,2000.0]  |\n|[3.73,56.0]     |\n+----------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\nfeatures        |\n+----------------+\n[25.95,1.0]     |\n[666.88,95.0]   |\n[297.69,29.0]   |\n[196.71,30.0]   |\n[972.12,175.0]  |\n[156.09,20.0]   |\n[507.36,65.0]   |\n[624.48,80.0]   |\n[86.6,10.0]     |\n[43.31,5.0]     |\n[216.52,25.0]   |\n[6.98,112.0]    |\n[212.1,7859.0]  |\n[33.81,336.0]   |\n[17.1,308.0]    |\n[17.74,112.0]   |\n[517.32,18305.0]|\n[49.46,2075.0]  |\n[83.53,2000.0]  |\n[3.73,56.0]     |\n+----------------+\nonly showing top 20 rows\n\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1320319968314220&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> <span class=\"ansi-red-fg\"># Trains a k-means model</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     24</span> kmeans <span class=\"ansi-blue-fg\">=</span> KMeans<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>setK<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>setSeed<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\"> </span>model <span class=\"ansi-blue-fg\">=</span> kmeans<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     26</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     27</span> <span class=\"ansi-red-fg\"># Make predictions</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    127</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 129</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    130</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    319</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    320</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 321</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    322</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    323</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    317</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 318</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    319</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    320</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o4223.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 533.0 failed 1 times, most recent failure: Lost task 0.0 in stage 533.0 (TID 1043, ip-10-172-180-105.us-west-2.compute.internal, executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/landing_fact_predescription/part-00000-4b238732-7d53-4b1d-b830-4d80733ca5a3-c000.snappy.parquet.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:328)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:481)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: can not read class org.apache.parquet.format.PageHeader: don&#39;t know what type: 13\n\tat org.apache.parquet.format.Util.read(Util.java:216)\n\tat org.apache.parquet.format.Util.readPageHeader(Util.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader$WorkaroundChunk.readPageHeader(ParquetFileReader.java:1110)\n\tat org.apache.parquet.hadoop.ParquetFileReader$Chunk.readAllPages(ParquetFileReader.java:977)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:822)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.checkEndOfRowGroup(DatabricksVectorizedParquetRecordReader.java:455)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:171)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:40)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:293)\n\t... 18 more\nCaused by: shaded.parquet.org.apache.thrift.protocol.TProtocolException: don&#39;t know what type: 13\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.getTType(TCompactProtocol.java:896)\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.readFieldBegin(TCompactProtocol.java:558)\n\tat org.apache.parquet.format.InterningProtocol.readFieldBegin(InterningProtocol.java:158)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:973)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:966)\n\tat org.apache.parquet.format.PageHeader.read(PageHeader.java:843)\n\tat org.apache.parquet.format.Util.read(Util.java:213)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2354)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2373)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1234)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeSample$1(RDD.scala:615)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:604)\n\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:367)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:257)\n\tat org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)\n\tat org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/landing_fact_predescription/part-00000-4b238732-7d53-4b1d-b830-4d80733ca5a3-c000.snappy.parquet.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:328)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:481)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: can not read class org.apache.parquet.format.PageHeader: don&#39;t know what type: 13\n\tat org.apache.parquet.format.Util.read(Util.java:216)\n\tat org.apache.parquet.format.Util.readPageHeader(Util.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader$WorkaroundChunk.readPageHeader(ParquetFileReader.java:1110)\n\tat org.apache.parquet.hadoop.ParquetFileReader$Chunk.readAllPages(ParquetFileReader.java:977)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:822)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.checkEndOfRowGroup(DatabricksVectorizedParquetRecordReader.java:455)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:171)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:40)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:293)\n\t... 18 more\nCaused by: shaded.parquet.org.apache.thrift.protocol.TProtocolException: don&#39;t know what type: 13\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.getTType(TCompactProtocol.java:896)\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.readFieldBegin(TCompactProtocol.java:558)\n\tat org.apache.parquet.format.InterningProtocol.readFieldBegin(InterningProtocol.java:158)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:973)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:966)\n\tat org.apache.parquet.format.PageHeader.read(PageHeader.java:843)\n\tat org.apache.parquet.format.Util.read(Util.java:213)\n\t... 27 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 533.0 failed 1 times, most recent failure: Lost task 0.0 in stage 533.0 (TID 1043, ip-10-172-180-105.us-west-2.compute.internal, executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/landing_fact_predescription/part-00000-4b238732-7d53-4b1d-b830-4d80733ca5a3-c000.snappy.parquet.","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1320319968314220&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     23</span> <span class=\"ansi-red-fg\"># Trains a k-means model</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     24</span> kmeans <span class=\"ansi-blue-fg\">=</span> KMeans<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>setK<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>setSeed<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">---&gt; 25</span><span class=\"ansi-red-fg\"> </span>model <span class=\"ansi-blue-fg\">=</span> kmeans<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     26</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     27</span> <span class=\"ansi-red-fg\"># Make predictions</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    127</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 129</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    130</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    319</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    320</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 321</span><span class=\"ansi-red-fg\">         </span>java_model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_fit_java<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    322</span>         model <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_create_model<span class=\"ansi-blue-fg\">(</span>java_model<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    323</span>         <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_copyValues<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansi-cyan-fg\">_fit_java</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    317</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 318</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    319</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    320</span>     <span class=\"ansi-green-fg\">def</span> _fit<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> dataset<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o4223.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 533.0 failed 1 times, most recent failure: Lost task 0.0 in stage 533.0 (TID 1043, ip-10-172-180-105.us-west-2.compute.internal, executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/landing_fact_predescription/part-00000-4b238732-7d53-4b1d-b830-4d80733ca5a3-c000.snappy.parquet.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:328)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:481)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: can not read class org.apache.parquet.format.PageHeader: don&#39;t know what type: 13\n\tat org.apache.parquet.format.Util.read(Util.java:216)\n\tat org.apache.parquet.format.Util.readPageHeader(Util.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader$WorkaroundChunk.readPageHeader(ParquetFileReader.java:1110)\n\tat org.apache.parquet.hadoop.ParquetFileReader$Chunk.readAllPages(ParquetFileReader.java:977)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:822)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.checkEndOfRowGroup(DatabricksVectorizedParquetRecordReader.java:455)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:171)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:40)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:293)\n\t... 18 more\nCaused by: shaded.parquet.org.apache.thrift.protocol.TProtocolException: don&#39;t know what type: 13\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.getTType(TCompactProtocol.java:896)\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.readFieldBegin(TCompactProtocol.java:558)\n\tat org.apache.parquet.format.InterningProtocol.readFieldBegin(InterningProtocol.java:158)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:973)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:966)\n\tat org.apache.parquet.format.PageHeader.read(PageHeader.java:843)\n\tat org.apache.parquet.format.Util.read(Util.java:213)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2354)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2373)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1234)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeSample$1(RDD.scala:615)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:604)\n\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:367)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:257)\n\tat org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231)\n\tat org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:354)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:329)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/user/hive/warehouse/landing_fact_predescription/part-00000-4b238732-7d53-4b1d-b830-4d80733ca5a3-c000.snappy.parquet.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:349)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:328)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:481)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:68)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:54)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:101)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:104)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.io.IOException: can not read class org.apache.parquet.format.PageHeader: don&#39;t know what type: 13\n\tat org.apache.parquet.format.Util.read(Util.java:216)\n\tat org.apache.parquet.format.Util.readPageHeader(Util.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader$WorkaroundChunk.readPageHeader(ParquetFileReader.java:1110)\n\tat org.apache.parquet.hadoop.ParquetFileReader$Chunk.readAllPages(ParquetFileReader.java:977)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readNextRowGroup(ParquetFileReader.java:822)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.checkEndOfRowGroup(DatabricksVectorizedParquetRecordReader.java:455)\n\tat com.databricks.sql.io.parquet.DatabricksVectorizedParquetRecordReader.nextBatch(DatabricksVectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:171)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:40)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:293)\n\t... 18 more\nCaused by: shaded.parquet.org.apache.thrift.protocol.TProtocolException: don&#39;t know what type: 13\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.getTType(TCompactProtocol.java:896)\n\tat shaded.parquet.org.apache.thrift.protocol.TCompactProtocol.readFieldBegin(TCompactProtocol.java:558)\n\tat org.apache.parquet.format.InterningProtocol.readFieldBegin(InterningProtocol.java:158)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:973)\n\tat org.apache.parquet.format.PageHeader$PageHeaderStandardScheme.read(PageHeader.java:966)\n\tat org.apache.parquet.format.PageHeader.read(PageHeader.java:843)\n\tat org.apache.parquet.format.Util.read(Util.java:213)\n\t... 27 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# https://spark.apache.org/docs/latest/ml-clustering.html\n# https://docs.databricks.com/applications/mlflow/quick-start-python.html\n\n#example k means clustering \n\ndesPd = spark.sql('select * from presentation_fact_description_gp')\n# to do set feature columns \n\nkmeans = KMeans().setK(2).setSeed(1)\nmodel = kmeans.fit(desPd)\npredictions = model.transform(desPd)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)\n# desPd.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b871ed67-6ee0-4494-ad5c-a9389d4c2825"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8417cb1-85ff-4b47-aeb6-91fd1c166d51"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dataAnalysis","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2161992318415424}},"nbformat":4,"nbformat_minor":0}
